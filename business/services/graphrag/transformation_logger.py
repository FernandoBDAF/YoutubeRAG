"""
Transformation Logger Service

This module provides structured logging for all GraphRAG pipeline transformations,
enabling "why" questions about entity merges, relationship filtering, and community formation.
"""

import logging
import time
import json
from typing import Dict, List, Any, Optional
from datetime import datetime, timezone
from pymongo.database import Database
from pymongo.collection import Collection
from core.libraries.error_handling.decorators import handle_errors

logger = logging.getLogger(__name__)


class TransformationLogger:
    """
    Service for logging all GraphRAG pipeline transformations with structured format.

    Logs are stored in MongoDB collection `transformation_logs` for queryability.
    Each log entry includes operation, stage, entity_id, reason, confidence, trace_id, timestamp.
    
    Performance Optimization (Achievement 7.2):
    - Uses batch writes with configurable buffer size
    - Automatic flush on buffer full or explicit flush call
    - Expected: 30-50% reduction in logging overhead
    """

    def __init__(self, db: Database, enabled: bool = True, batch_size: int = 100):
        """
        Initialize the Transformation Logger.

        Args:
            db: MongoDB database instance
            enabled: Whether logging is enabled (can disable for performance)
            batch_size: Number of log entries to buffer before flushing (default: 100)
        """
        self.db = db
        self.enabled = enabled
        self.batch_size = batch_size
        self.collection: Optional[Collection] = None
        self._buffer: List[Dict[str, Any]] = []

        if self.enabled:
            self.collection = db.transformation_logs
            # Create indexes for fast querying
            self._ensure_indexes()

        logger.info(f"Initialized TransformationLogger (enabled={enabled}, batch_size={batch_size})")

    def _ensure_indexes(self):
        """Create indexes for fast querying."""
        if self.collection is None:
            return

        try:
            # Index for trace_id queries (most common)
            self.collection.create_index("trace_id")
            # Index for entity_id queries
            self.collection.create_index("entity_id")
            # Index for stage queries
            self.collection.create_index("stage")
            # Index for operation queries
            self.collection.create_index("operation")
            # Compound index for common queries
            self.collection.create_index([("trace_id", 1), ("stage", 1)])
            self.collection.create_index([("trace_id", 1), ("entity_id", 1)])
            # Index for timestamp queries
            self.collection.create_index("timestamp")
            logger.debug("Created transformation_logs indexes")
        except Exception as e:
            logger.warning(f"Failed to create transformation_logs indexes: {e}")

    def flush_buffer(self) -> int:
        """
        Flush buffered log entries to MongoDB using batch write.
        
        Performance Optimization (Achievement 7.2):
        - Uses insert_many() instead of multiple insert_one() calls
        - Reduces network round-trips to MongoDB
        
        Returns:
            Number of log entries flushed
        """
        if not self.enabled or not self._buffer:
            return 0
        
        count = len(self._buffer)
        if count == 0:
            return 0
            
        # Create a copy of buffer to avoid clearing while insert_many is processing
        buffer_copy = list(self._buffer)
        self._buffer.clear()
        
        try:
            self.collection.insert_many(buffer_copy, ordered=False)
            logger.debug(f"Flushed {count} transformation log entries")
        except Exception as e:
            logger.error(f"Failed to flush transformation log buffer: {e}")
            # Re-add failed entries to buffer for retry
            self._buffer.extend(buffer_copy)
            raise
        
        return count
    
    def __del__(self):
        """Destructor: Ensure buffer is flushed when logger is destroyed."""
        try:
            self.flush_buffer()
        except:
            pass  # Ignore errors during cleanup

    def _log_transformation(
        self,
        operation: str,
        stage: str,
        data: Dict[str, Any],
        trace_id: str,
    ) -> Optional[str]:
        """
        Internal method to log a transformation.
        
        Performance Optimization (Achievement 7.2):
        - Buffers log entries and flushes in batches
        - Automatic flush when buffer reaches batch_size
        - Reduces MongoDB write overhead by 30-50%

        Args:
            operation: Operation type (MERGE, CREATE, SKIP, RELATIONSHIP, FILTER, AUGMENT, COMMUNITY, CLUSTER)
            stage: Stage name (entity_resolution, graph_construction, community_detection)
            data: Operation-specific data (entity_id, reason, confidence, etc.)
            trace_id: Trace ID linking transformations across stages

        Returns:
            "buffered" if buffered, None if disabled
        """
        if not self.enabled:
            return None

        try:
            log_entry = {
                "trace_id": trace_id,
                "stage": stage,
                "operation": operation,
                "timestamp": time.time(),
                "datetime": datetime.now(timezone.utc).isoformat(),
                **data,  # Include all operation-specific data
            }

            # Add to buffer
            self._buffer.append(log_entry)

            # Also log to standard logger for human-readable format
            human_readable = self._format_human_readable(operation, stage, data, trace_id)
            logger.info(human_readable)

            # Flush buffer if it reaches batch_size
            if len(self._buffer) >= self.batch_size:
                self.flush_buffer()

            return "buffered"

        except Exception as e:
            logger.error(f"Failed to log transformation: {e}")
            return None

    def _format_human_readable(
        self, operation: str, stage: str, data: Dict[str, Any], trace_id: str
    ) -> str:
        """
        Format log entry as human-readable string.

        Args:
            operation: Operation type
            stage: Stage name
            data: Operation-specific data
            trace_id: Trace ID

        Returns:
            Human-readable log string
        """
        # Base format: "OPERATION: details | trace_id: X"
        parts = [f"{operation}:"]

        # Add operation-specific details
        if operation == "MERGE":
            entity_a = data.get("entity_a", {}).get("name", "unknown")
            entity_b = data.get("entity_b", {}).get("name", "unknown")
            entity_a_id = data.get("entity_a", {}).get("id", "unknown")[:8]
            entity_b_id = data.get("entity_b", {}).get("id", "unknown")[:8]
            reason = data.get("reason", "unknown")
            similarity = data.get("similarity", 0.0)
            confidence = data.get("confidence", 0.0)
            parts.append(
                f"{entity_a} ({entity_a_id}) → {entity_b} ({entity_b_id}) | "
                f"reason: {reason} | similarity: {similarity:.2f} | confidence: {confidence:.2f}"
            )
        elif operation == "CREATE":
            entity_name = data.get("entity", {}).get("name", "unknown")
            entity_id = data.get("entity", {}).get("id", "unknown")[:8]
            entity_type = data.get("entity_type", "unknown")
            sources = data.get("sources", 0)
            confidence = data.get("confidence", 0.0)
            parts.append(
                f"entity '{entity_name}' ({entity_id}) | "
                f"type: {entity_type} | sources: {sources} chunks | confidence: {confidence:.2f}"
            )
        elif operation == "SKIP":
            entity_name = data.get("entity", {}).get("name", "unknown")
            reason = data.get("reason", "unknown")
            confidence = data.get("confidence", 0.0)
            parts.append(
                f"entity '{entity_name}' | reason: {reason} | confidence: {confidence:.2f}"
            )
        elif operation == "RELATIONSHIP":
            subject = data.get("subject", {}).get("name", "unknown")
            predicate = data.get("predicate", "unknown")
            obj = data.get("object", {}).get("name", "unknown")
            rel_type = data.get("relationship_type", "unknown")
            confidence = data.get("confidence", 0.0)
            parts.append(
                f"{subject} → {predicate} → {obj} | "
                f"type: {rel_type} | confidence: {confidence:.2f}"
            )
        elif operation == "FILTER":
            subject = data.get("subject", {}).get("name", "unknown")
            predicate = data.get("predicate", "unknown")
            obj = data.get("object", {}).get("name", "unknown")
            reason = data.get("reason", "unknown")
            confidence = data.get("confidence", 0.0)
            threshold = data.get("threshold", 0.0)
            parts.append(
                f"relationship dropped | {subject} → {predicate} → {obj} | "
                f"reason: {reason} | confidence: {confidence:.2f} | threshold: {threshold:.2f}"
            )
        elif operation == "AUGMENT":
            subject = data.get("subject", {}).get("name", "unknown")
            obj = data.get("object", {}).get("name", "unknown")
            method = data.get("method", "unknown")
            chunk_id = data.get("chunk_id", "unknown")[:8] if data.get("chunk_id") else "N/A"
            confidence = data.get("confidence", 0.0)
            parts.append(
                f"added {method} link | entities: ({subject}, {obj}) | "
                f"chunk: {chunk_id} | confidence: {confidence:.2f}"
            )
        elif operation == "COMMUNITY":
            community_id = data.get("community_id", "unknown")
            entity_count = data.get("entity_count", 0)
            modularity = data.get("modularity", 0.0)
            coherence = data.get("coherence", 0.0)
            parts.append(
                f"formed {community_id} | entities: {entity_count} | "
                f"modularity: {modularity:.2f} | coherence: {coherence:.2f}"
            )
        elif operation == "CLUSTER":
            entity_name = data.get("entity", {}).get("name", "unknown")
            entity_id = data.get("entity", {}).get("id", "unknown")[:8]
            community_id = data.get("community_id", "unknown")
            reason = data.get("reason", "unknown")
            neighbors = data.get("neighbors", 0)
            parts.append(
                f"{entity_name} ({entity_id}) assigned to {community_id} | "
                f"reason: {reason} | neighbors: {neighbors}"
            )

        # Add trace_id
        parts.append(f"| trace_id: {trace_id}")

        return " ".join(parts)

    # Entity Resolution Logging Methods

    def log_entity_merge(
        self,
        entity_a: Dict[str, Any],
        entity_b: Dict[str, Any],
        result_entity: Dict[str, Any],
        reason: str,
        similarity: float,
        confidence: float,
        method: Optional[str] = None,
        trace_id: str = "unknown",
    ) -> Optional[str]:
        """
        Log entity merge operation.

        Args:
            entity_a: First entity (dict with id, name)
            entity_b: Second entity (dict with id, name)
            result_entity: Resulting merged entity (dict with id, name)
            reason: Merge reason (fuzzy_match, embedding_match, context_match)
            similarity: Similarity score (0.0-1.0)
            confidence: Confidence score (0.0-1.0)
            method: Matching method used (levenshtein, jaro_winkler, embedding, etc.)
            trace_id: Trace ID

        Returns:
            Log entry ID if logged, None if disabled
        """
        data = {
            "entity_a": entity_a,
            "entity_b": entity_b,
            "result_entity": result_entity,
            "reason": reason,
            "similarity": similarity,
            "confidence": confidence,
        }
        if method:
            data["method"] = method

        return self._log_transformation("MERGE", "entity_resolution", data, trace_id)

    def log_entity_create(
        self,
        entity: Dict[str, Any],
        entity_type: str,
        sources: int,
        confidence: float,
        trace_id: str = "unknown",
    ) -> Optional[str]:
        """
        Log entity creation operation.

        Args:
            entity: Entity dict (with id, name)
            entity_type: Entity type (PERSON, TECHNOLOGY, etc.)
            sources: Number of source chunks
            confidence: Confidence score (0.0-1.0)
            trace_id: Trace ID

        Returns:
            Log entry ID if logged, None if disabled
        """
        data = {
            "entity": entity,
            "entity_type": entity_type,
            "sources": sources,
            "confidence": confidence,
        }

        return self._log_transformation("CREATE", "entity_resolution", data, trace_id)

    def log_entity_skip(
        self,
        entity: Dict[str, Any],
        reason: str,
        confidence: float,
        trace_id: str = "unknown",
    ) -> Optional[str]:
        """
        Log entity skip operation.

        Args:
            entity: Entity dict (with id, name)
            reason: Skip reason (stopword, low_confidence, invalid_type, etc.)
            confidence: Confidence score (0.0-1.0)
            trace_id: Trace ID

        Returns:
            Log entry ID if logged, None if disabled
        """
        data = {
            "entity": entity,
            "reason": reason,
            "confidence": confidence,
        }

        return self._log_transformation("SKIP", "entity_resolution", data, trace_id)

    # Graph Construction Logging Methods

    def log_relationship_create(
        self,
        relationship: Dict[str, Any],
        relationship_type: str,
        confidence: float,
        trace_id: str = "unknown",
    ) -> Optional[str]:
        """
        Log relationship creation operation.

        Args:
            relationship: Relationship dict (with subject, predicate, object)
            relationship_type: Relationship type (llm_extracted, co_occurrence, semantic_similarity, etc.)
            confidence: Confidence score (0.0-1.0)
            trace_id: Trace ID

        Returns:
            Log entry ID if logged, None if disabled
        """
        data = {
            "subject": relationship.get("subject", {}),
            "predicate": relationship.get("predicate", ""),
            "object": relationship.get("object", {}),
            "relationship_type": relationship_type,
            "confidence": confidence,
        }

        return self._log_transformation("RELATIONSHIP", "graph_construction", data, trace_id)

    def log_relationship_filter(
        self,
        relationship: Dict[str, Any],
        reason: str,
        confidence: float,
        threshold: float,
        trace_id: str = "unknown",
    ) -> Optional[str]:
        """
        Log relationship filter operation.

        Args:
            relationship: Relationship dict (with subject, predicate, object)
            reason: Filter reason (below_threshold, invalid_predicate, etc.)
            confidence: Confidence score (0.0-1.0)
            threshold: Threshold used for filtering
            trace_id: Trace ID

        Returns:
            Log entry ID if logged, None if disabled
        """
        data = {
            "subject": relationship.get("subject", {}),
            "predicate": relationship.get("predicate", ""),
            "object": relationship.get("object", {}),
            "reason": reason,
            "confidence": confidence,
            "threshold": threshold,
        }

        return self._log_transformation("FILTER", "graph_construction", data, trace_id)

    def log_relationship_augment(
        self,
        relationship: Dict[str, Any],
        method: str,
        confidence: float,
        chunk_id: Optional[str] = None,
        trace_id: str = "unknown",
    ) -> Optional[str]:
        """
        Log relationship augmentation operation.

        Args:
            relationship: Relationship dict (with subject, predicate, object)
            method: Augmentation method (co_occurrence, semantic_similarity, cross_chunk, bidirectional)
            confidence: Confidence score (0.0-1.0)
            chunk_id: Chunk ID where relationship was found (if applicable)
            trace_id: Trace ID

        Returns:
            Log entry ID if logged, None if disabled
        """
        data = {
            "subject": relationship.get("subject", {}),
            "predicate": relationship.get("predicate", ""),
            "object": relationship.get("object", {}),
            "method": method,
            "confidence": confidence,
        }
        if chunk_id:
            data["chunk_id"] = chunk_id

        return self._log_transformation("AUGMENT", "graph_construction", data, trace_id)

    # Community Detection Logging Methods

    def log_community_form(
        self,
        community_id: str,
        entities: List[Dict[str, Any]],
        modularity: float,
        coherence: float,
        algorithm: Optional[str] = None,
        resolution_parameter: Optional[float] = None,
        trace_id: str = "unknown",
    ) -> Optional[str]:
        """
        Log community formation operation.

        Args:
            community_id: Community ID
            entities: List of entity dicts in community
            modularity: Modularity score
            coherence: Coherence score
            algorithm: Algorithm used (louvain, leiden, infomap)
            resolution_parameter: Resolution parameter used
            trace_id: Trace ID

        Returns:
            Log entry ID if logged, None if disabled
        """
        data = {
            "community_id": community_id,
            "entity_count": len(entities),
            "entity_ids": [e.get("id", "") for e in entities],
            "modularity": modularity,
            "coherence": coherence,
        }
        if algorithm:
            data["algorithm"] = algorithm
        if resolution_parameter is not None:
            data["resolution_parameter"] = resolution_parameter

        return self._log_transformation("COMMUNITY", "community_detection", data, trace_id)

    def log_entity_cluster(
        self,
        entity: Dict[str, Any],
        community_id: str,
        reason: str,
        neighbors: int,
        trace_id: str = "unknown",
    ) -> Optional[str]:
        """
        Log entity cluster assignment operation.

        Args:
            entity: Entity dict (with id, name)
            community_id: Community ID entity was assigned to
            reason: Assignment reason (high_edge_weight, algorithm_decision, etc.)
            neighbors: Number of neighbors in community
            trace_id: Trace ID

        Returns:
            Log entry ID if logged, None if disabled
        """
        data = {
            "entity": entity,
            "community_id": community_id,
            "reason": reason,
            "neighbors": neighbors,
        }

        return self._log_transformation("CLUSTER", "community_detection", data, trace_id)

    # Query Methods

    @handle_errors(fallback=[], log_traceback=True, reraise=False)
    def get_transformations_by_trace_id(
        self, trace_id: str, stage: Optional[str] = None, operation: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Get all transformations for a specific trace_id.

        Args:
            trace_id: Trace ID to query
            stage: Optional stage filter
            operation: Optional operation filter

        Returns:
            List of transformation log entries
        """
        if self.collection is None:
            return []

        query = {"trace_id": trace_id}
        if stage:
            query["stage"] = stage
        if operation:
            query["operation"] = operation

        cursor = self.collection.find(query).sort("timestamp", 1)
        return list(cursor)

    @handle_errors(fallback=[], log_traceback=True, reraise=False)
    def get_transformations_by_entity_id(
        self, entity_id: str, trace_id: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Get all transformations for a specific entity_id.

        Args:
            entity_id: Entity ID to query
            trace_id: Optional trace ID filter

        Returns:
            List of transformation log entries
        """
        if self.collection is None:
            return []

        # Query for entity_id in various fields (entity_a.id, entity_b.id, entity.id, result_entity.id)
        query = {
            "$or": [
                {"entity_a.id": entity_id},
                {"entity_b.id": entity_id},
                {"entity.id": entity_id},
                {"result_entity.id": entity_id},
                {"subject.id": entity_id},
                {"object.id": entity_id},
            ]
        }
        if trace_id:
            query["trace_id"] = trace_id

        cursor = self.collection.find(query).sort("timestamp", 1)
        return list(cursor)

    @handle_errors(fallback=[], log_traceback=True, reraise=False)
    def get_transformations_by_stage(
        self, stage: str, trace_id: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Get all transformations for a specific stage.

        Args:
            stage: Stage name (entity_resolution, graph_construction, community_detection)
            trace_id: Optional trace ID filter

        Returns:
            List of transformation log entries
        """
        if self.collection is None:
            return []

        query = {"stage": stage}
        if trace_id:
            query["trace_id"] = trace_id

        cursor = self.collection.find(query).sort("timestamp", 1)
        return list(cursor)


def get_transformation_logger(db: Database, enabled: bool = True) -> TransformationLogger:
    """
    Get or create TransformationLogger instance.

    Args:
        db: MongoDB database instance
        enabled: Whether logging is enabled

    Returns:
        TransformationLogger instance
    """
    # Check environment variable for global enable/disable
    import os

    env_enabled = os.getenv("GRAPHRAG_TRANSFORMATION_LOGGING", "true").lower() == "true"
    final_enabled = enabled and env_enabled

    return TransformationLogger(db, enabled=final_enabled)
