# Concurrent Processing with Batch Safety

**Date**: November 4, 2025  
**Issue**: Original implementation waited until ALL chunks extracted before writing  
**Risk**: Lose everything if process crashes during extraction  
**Solution**: Write results in batches of 100

---

## ‚ö†Ô∏è **The Problem You Identified**

**Original Design** (DANGEROUS):

```python
# Extract ALL chunks (could take hours)
results = run_llm_concurrent(all_13k_chunks...)

# THEN write to DB (at the very end)
store_results(results)
```

**Risks**:

- Process crashes ‚Üí Lose ALL extractions
- Memory overflow ‚Üí Lose ALL extractions
- Network issue ‚Üí Lose ALL extractions
- Server restart ‚Üí Lose ALL extractions

**Example**: Extract for 6 hours, crash at 5h 59min ‚Üí **Lost 6 hours of work** ‚ùå

---

## ‚úÖ **The Fix - Incremental Batch Writes**

**New Design** (SAFE):

```python
# Process in batches of 100
for batch in chunks_in_batches_of_100:
    # Extract batch (1-2 minutes)
    results = run_llm_concurrent(batch_100_chunks...)

    # Write batch IMMEDIATELY (saves progress)
    store_results(batch, results)

# If crash occurs:
# - Already wrote 700 chunks
# - Only lose current batch (100 chunks max)
# - Can resume from chunk 800
```

**Benefits**:

- ‚úÖ Maximum data loss: 100 chunks (vs 13,000)
- ‚úÖ Can resume from last batch
- ‚úÖ See progress in database
- ‚úÖ Memory efficient

---

## üìä **Batch Size Analysis**

| Batch Size | Write Frequency | Max Data Loss  | Memory Usage | Overhead    |
| ---------- | --------------- | -------------- | ------------ | ----------- |
| 1          | Every chunk     | 1 chunk        | Low          | High (slow) |
| 50         | Every 50        | 50 chunks      | Low          | Medium      |
| **100**    | **Every 100**   | **100 chunks** | **Medium**   | **Low** ‚≠ê  |
| 500        | Every 500       | 500 chunks     | High         | Very Low    |
| All (13k)  | Once at end     | 13,000 chunks  | Very High    | None ‚ùå     |

**Chosen**: 100 chunks per batch (good balance)

---

## üéØ **How It Works Now**

### For 1,000 Chunks

**Execution**:

```
Batch 1/10: Processing chunks 1-100
  - Extract 100 chunks concurrently
  - Write 100 results to DB ‚úÖ
  - Progress saved!

Batch 2/10: Processing chunks 101-200
  - Extract 100 chunks concurrently
  - Write 100 results to DB ‚úÖ
  - Progress saved!

... (continue)

Batch 10/10: Processing chunks 901-1000
  - Extract 100 chunks concurrently
  - Write 100 results to DB ‚úÖ
  - Complete!
```

**If Crash at Batch 7**:

- ‚úÖ Batches 1-6 saved (600 chunks)
- ‚ùå Batch 7 lost (100 chunks)
- ‚è≥ Batches 8-10 not started (300 chunks)
- **Recovery**: Re-run from chunk 601, only lose 100 chunks

---

## üí° **Additional Safety Features**

### 1. Progress Logging

```
Batch 1/10: Processing chunks 1-100
Batch 1 complete: 95 updated, 5 failed so far
Batch 2/10: Processing chunks 101-200
Batch 2 complete: 192 updated, 8 failed so far
...
```

**Benefit**: Know exactly where you are

---

### 2. Partial Results on Error

```python
except Exception as e:
    logger.info(
        f"Partial results saved: {self.stats['updated']} updated before error"
    )
```

**Benefit**: Can resume with `--skip N` or query DB for processed chunks

---

### 3. Resume Capability

**After crash, can resume**:

```bash
# Check what was processed
python -c "
from pymongo import MongoClient
import os
from dotenv import load_dotenv

load_dotenv()
client = MongoClient(os.getenv('MONGODB_URI'))
db = client['validation_db']

processed = db.video_chunks.count_documents({'graphrag_extraction': {'$exists': True}})
print(f'Already processed: {processed} chunks')
print(f'To resume, run with query that skips processed chunks')
"

# Resume (extraction stage already handles this)
python -m app.cli.graphrag --stage graph_extraction \
  --concurrency 10 \
  --read-db-name validation_db \
  --write-db-name validation_db
# Will automatically skip chunks with extraction status
```

---

## üìä **Performance Impact**

**Overhead from Batching**: Minimal

- Write operations: ~1 second per 100 chunks
- Total overhead: 10 seconds for 1,000 chunks
- **Cost**: <1% slowdown
- **Benefit**: 99% data safety

**Tradeoff**: Worth it!

---

## ‚úÖ **What Changed**

**Before** (your concern):

- Extract all ‚Üí Write all at end
- Risk: Lose everything

**After** (fixed):

- Extract batch ‚Üí Write batch ‚Üí Repeat
- Risk: Lose at most 100 chunks
- Safety: 99% of work preserved

---

**Your instinct was correct** - the original design was dangerous for large datasets. The fix ensures incremental progress saves. ‚úÖ
