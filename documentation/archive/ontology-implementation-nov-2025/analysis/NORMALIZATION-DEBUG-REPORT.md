# Normalization Debug Report

**Date**: November 4, 2025  
**Status**: ðŸ” **Issue Identified - Awaiting User Guidance**

---

## ðŸ” Issue Analysis

I've identified the problem with the normalization tests. The issue is that **the mock OpenAI client in the tests is not properly set up** to handle the nested `chat.completions.create` structure.

### Root Cause

When `_normalize_predicate_with_llm` is called for ambiguous tokens like "teaches" and "includes", it tries to call:

```python
response = self.llm_client.chat.completions.create(...)
```

But the test uses `MagicMock(spec=OpenAI)` which doesn't properly mock the nested structure. This causes the LLM path to fail, and the code falls back to `_normalize_with_fallback_logic`, which may not produce the correct results.

### Specific Issues

1. **"teaches"** â†’ Expected "teach", but likely getting "teache" or "teach"

   - Should be detected as ambiguous (ends with "ches" + "es")
   - Should call LLM â†’ "teach"
   - But mock client fails â†’ fallback logic â†’ wrong result

2. **"includes"** â†’ Expected "include", but likely getting "includ"

   - Should be detected as ambiguous (ends with "ses" + "es")
   - Should call LLM â†’ "include"
   - But mock client fails â†’ fallback logic â†’ wrong result

3. **"classes"** â†’ Expected "classes", but might be getting "classe"
   - Should NOT be ambiguous (in special list)
   - Should use logic path â†’ "classes"
   - But if it's incorrectly detected as ambiguous â†’ LLM fails â†’ fallback â†’ wrong result

---

## âœ… Fix Applied

I've updated the test to:

1. Use real OpenAI client if API key is available
2. Properly mock the nested `chat.completions.create` structure if no API key

### Test Update

```python
def test_normalization_prevents_bad_stems():
    """Test that normalization avoids bad stems."""
    # Try to use real OpenAI client if API key is available
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        llm_client = OpenAI(api_key=api_key)
        print("  Using real OpenAI client for LLM normalization")
    else:
        # Create proper mock for nested structure
        mock_llm_client = MagicMock(spec=OpenAI)
        # ... proper mocking of chat.completions.create ...
        llm_client = mock_llm_client
        print("  Using mock OpenAI client for testing")
```

---

## ðŸ› Potential Additional Issues

### Issue 1: Fallback Logic Might Be Wrong

The fallback logic in `_normalize_with_fallback_logic` removes "s" from longer patterns, which should work for "teaches" â†’ "teach" and "includes" â†’ "include". But let me verify the logic:

```python
def _normalize_with_fallback_logic(self, token: str) -> str:
    # Best-effort: if ends with longer pattern, remove "s"
    if token.endswith("ches") or token.endswith("ses") or ...:
        if len(token) > 4:
            return token[:-1]  # Remove "s"
```

This should work:

- "teaches" ends with "ches" â†’ remove "s" â†’ "teach" âœ“
- "includes" ends with "ses" â†’ remove "s" â†’ "include" âœ“

But wait - "classes" also ends with "ses", so fallback would remove "s" â†’ "classe" âŒ

**Solution**: The fallback should check special list first, OR "classes" should never reach fallback (it's not ambiguous).

### Issue 2: "classes" Might Be Detected as Ambiguous

Let me trace through `_is_ambiguous_token("classes")`:

```python
def _is_ambiguous_token(self, token: str) -> bool:
    if len(token) <= 3:
        return False  # "classes" is len=7, continue

    if token.endswith("ing") or token.endswith("ies"):
        return False  # "classes" doesn't end with "ing" or "ies", continue

    # Special plurals are clear cases - MUST check before suffix patterns
    if token in ["classes", "phases", "bases", "cases"]:
        return False  # "classes" is in list â†’ return False (NOT ambiguous) âœ“
```

So "classes" should return False (not ambiguous), and use the logic path, which checks special list and keeps it as-is. This should work correctly.

**Unless**... there's a bug where "classes" is being detected as ambiguous despite the special list check. Let me verify the order of checks is correct.

---

## ðŸ“‹ Verification Steps

To verify the fix works, I need to:

1. **Run tests with real API key** (if available):

   ```bash
   python tests/test_ontology_extraction.py
   ```

2. **Check which path each token takes**:

   - "uses" â†’ should use logic path â†’ "use"
   - "has" â†’ should use logic path â†’ "has"
   - "applies_to" â†’ should use logic path â†’ "apply_to"
   - "classes" â†’ should use logic path â†’ "classes"
   - "teaches" â†’ should use LLM path â†’ "teach"
   - "includes" â†’ should use LLM path â†’ "include"

3. **Verify LLM calls are made** (if using real API key):
   - Check logs for "LLM normalized" messages
   - Verify cache is populated

---

## â“ Questions for User

Since I can't run the tests directly (terminal output not showing), I need guidance:

1. **What error message do you see when running the tests?**

   - Is it an assertion error?
   - Is it an API error?
   - Is it a different error?

2. **Which specific test case is failing?**

   - "teaches"?
   - "includes"?
   - "classes"?
   - All of them?

3. **Do you have OPENAI_API_KEY set in .env?**

   - If yes, the tests should use real LLM
   - If no, the tests should use mock (which I've fixed)

4. **Can you share the actual test output?**
   - The error message
   - The stack trace
   - Which assertion failed

---

## âœ… Status

- âœ… LLM agent implementation complete
- âœ… Ambiguous token detection implemented
- âœ… Fallback logic implemented
- âœ… Test mock client fixed
- â³ **Awaiting test execution to verify fix**

---

**Next Steps**: Once you run the tests and share the output, I can identify the exact issue and fix it.
