Execute Achievement 0.2 in @PLAN_METHODOLOGY-V2-ENHANCEMENTS.md following strict methodology.

(Note: We're inserting this as Achievement 0.2, shifting other achievements)

═══════════════════════════════════════════════════════════════════════

CONTEXT BOUNDARIES (Read ONLY These):
✅ @EXECUTION_ANALYSIS_PROMPT-AUTOMATION-STRATEGY.md - Implementation section (200 lines)
✅ @EXECUTION_ANALYSIS_IDEAL-PROMPT-EXAMPLE.md - Template structure (100 lines)
✅ @PLAN_METHODOLOGY-V2-ENHANCEMENTS.md - Achievement 0.2 goal (50 lines)

❌ DO NOT READ: Full PLAN (721 lines), other achievements, failure analysis docs

CONTEXT BUDGET: 350 lines maximum

═══════════════════════════════════════════════════════════════════════

ACHIEVEMENT 0.2: Automated Prompt Generation Tool

Goal: Create script that generates ideal prompts automatically in <5 seconds
Estimated: 3-4 hours
Priority: CRITICAL (makes all remaining work easier)

This is HIGH PRIORITY because:
- User identified manual prompt creation as impractical
- Will use for remaining 9 achievements (0.1, 1.1-6.1)
- Saves 8 min × 10 achievements = 80 minutes
- Tests automation approach immediately

═══════════════════════════════════════════════════════════════════════

REQUIRED STEPS (No Shortcuts):

Step 1: Create SUBPLAN (MANDATORY)
- File: SUBPLAN_METHODOLOGY-V2-ENHANCEMENTS_02.md
- Size: 200-400 lines
- Must include:
  - Objective: Build generate_prompt.py with parsing and templates
  - Deliverables: Script file, templates, tests
  - Approach: Parse PLAN → Calculate context → Fill template → Output
  - Tests: Test with current PLAN, verify output matches ideal
  - Expected Results: One command generates perfect prompt

Step 2: Create EXECUTION_TASK (MANDATORY)
- File: EXECUTION_TASK_METHODOLOGY-V2-ENHANCEMENTS_02_01.md
- Size: 100-200 lines maximum
- Start with:
  - Objective: Build prompt generator script
  - Approach: Phase 1 (parse), Phase 2 (templates), Phase 3 (test)
  - Iteration Log: Iteration 1 (in progress)

Step 3: Execute Work (Build the Script)

Phase 1: Core Parsing (1h)
- Create: LLM/scripts/generate_prompt.py
- Functions:
  - parse_plan_file(path) → extracts achievements
  - find_next_achievement(feature) → detects what's next
  - calculate_context_lines(plan, achievement) → line counts
  - detect_validation_scripts() → which scripts exist

Phase 2: Template System (1.5h)
- Embed templates in script:
  - ACHIEVEMENT_EXECUTION_TEMPLATE (from ideal prompt)
  - CONTINUE_EXECUTION_TEMPLATE
  - NEXT_ACHIEVEMENT_TEMPLATE
- Function: fill_template(template, context) → complete prompt

Phase 3: CLI & Output (0.5h)
- Argparse: --next, --achievement N.N, --continue, --clipboard
- Output: Print prompt to stdout
- Clipboard: Copy if --clipboard flag

Phase 4: Testing (0.5-1h)
- Test: Generate prompt for Achievement 1.1
- Verify: Matches ideal prompt structure
- Test: Generate for Achievement 0.1 (archive case study)
- Verify: Context boundaries accurate

Step 4: Verify Deliverables (MANDATORY)
Run verification:
  ls -1 LLM/scripts/generate_prompt.py
  python LLM/scripts/generate_prompt.py --help
  python LLM/scripts/generate_prompt.py @PLAN_METHODOLOGY-V2-ENHANCEMENTS.md --next
  
If script doesn't exist or doesn't run: FIX before continuing
If output doesn't match ideal structure: FIX

Step 5: Complete EXECUTION_TASK
- Update: Iteration Log with "Complete" status
- Add: Learning Summary (what was learned building generator)
- Add: Completion Status (deliverable exists and works)
- Verify: File is <200 lines (check: wc -l EXECUTION_TASK_*.md)
- If >200: Must be too detailed, condense

Step 6: Archive Immediately (NEW PRACTICE!)
- Move: SUBPLAN_METHODOLOGY-V2-ENHANCEMENTS_02.md → methodology-v2-enhancements-archive/subplans/
- Move: EXECUTION_TASK_METHODOLOGY-V2-ENHANCEMENTS_02_01.md → methodology-v2-enhancements-archive/execution/
- Update: PLAN Subplan Tracking section

Step 7: Update PLAN
- Add Achievement 0.2 to Achievement Addition Log (dynamic addition!)
- Update Subplan Tracking:
  - SUBPLANs: 1 created (0 active, 1 archived)
  - EXECUTION_TASKs: 1 created (1 complete)
  - Time Spent: [from EXECUTION_TASK]
- Update "Current Status": Achievement 0.2 complete, next is 0.1

═══════════════════════════════════════════════════════════════════════

VALIDATION ENFORCEMENT (Will Be Built in This Achievement!):

After Step 4, run the generator itself to verify:
  python LLM/scripts/generate_prompt.py @PLAN_METHODOLOGY-V2-ENHANCEMENTS.md --achievement 1.1

Expected output: Complete prompt for Achievement 1.1
If output is missing elements: FIX script

After Step 5, size check:
✓ wc -l EXECUTION_TASK_METHODOLOGY-V2-ENHANCEMENTS_02_01.md
  - Must be <200 lines
  - If >200: Condense iteration log

═══════════════════════════════════════════════════════════════════════

DELIVERABLES CHECKLIST:

Must create:
- [ ] LLM/scripts/generate_prompt.py (Python script, 300-400 lines)
- [ ] Script has: parse_plan_file(), find_next_achievement(), calculate_context_lines(), detect_validation_scripts(), fill_template()
- [ ] Templates embedded: ACHIEVEMENT_EXECUTION_TEMPLATE, CONTINUE_EXECUTION_TEMPLATE
- [ ] CLI working: --next, --achievement, --clipboard flags
- [ ] Tested: Generates valid prompt for Achievement 1.1
- [ ] SUBPLAN_METHODOLOGY-V2-ENHANCEMENTS_02.md (approach documented)
- [ ] EXECUTION_TASK_METHODOLOGY-V2-ENHANCEMENTS_02_01.md (<200 lines, learnings)

═══════════════════════════════════════════════════════════════════════

DO NOT:
❌ Skip SUBPLAN ("it's just a script" - NO, all work needs SUBPLANs)
❌ Skip EXECUTION_TASK ("just write the code" - NO, track iterations)
❌ Mark complete without testing (must run and produce valid output)
❌ Read full PLAN (read Achievement 0.2 section + automation strategy doc only)
❌ Make script >400 lines (keep focused - just prompt generation)
❌ Skip clipboard support (--clipboard flag is essential for UX)

REMEMBER:
✓ This script will be used for ALL remaining achievements
✓ Quality is critical (bad generator = bad prompts = failures)
✓ Test with current PLAN (generate prompt for Achievement 1.1)
✓ Verify output matches ideal prompt structure
✓ EXECUTION_TASK must be <200 lines (focus on key learnings only)

═══════════════════════════════════════════════════════════════════════

EXTERNAL VERIFICATION:

After completing Achievement 0.2, I will verify:
1. Does LLM/scripts/generate_prompt.py exist?
2. Can I run: python LLM/scripts/generate_prompt.py --help (works?)
3. Can I run: python LLM/scripts/generate_prompt.py @PLAN_METHODOLOGY-V2-ENHANCEMENTS.md --next (produces prompt?)
4. Does output have all required sections? (context boundaries, steps, validation, DO NOTs)
5. Does SUBPLAN file exist with approach?
6. Does EXECUTION_TASK file exist with learnings?
7. Is EXECUTION_TASK <200 lines?

Do not proceed to Achievement 0.1 until I confirm verification passes.

═══════════════════════════════════════════════════════════════════════

SPECIAL NOTE:

This achievement is META - you're building the tool that will generate prompts for future achievements. This means:

- The prompt you're following now is the OUTPUT your script should produce
- Test your script by generating prompt for Achievement 1.1
- Compare generated prompt to this prompt structure
- They should be nearly identical (same sections, same safeguards)

═══════════════════════════════════════════════════════════════════════

Now beginning Achievement 0.2 execution:

Reading relevant sections only (350 lines):
- PROMPT-AUTOMATION-STRATEGY.md implementation section
- IDEAL-PROMPT-EXAMPLE.md template structure
- PLAN Achievement 0.2 goals

Creating SUBPLAN_METHODOLOGY-V2-ENHANCEMENTS_02.md...

EOF

